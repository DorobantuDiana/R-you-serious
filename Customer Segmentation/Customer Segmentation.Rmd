---
title: "Customer Segmentation"
author: "Diana Dorobantu"
date: "April 15, 2023"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This project aims to use clustering algorithms to segment customers based on their purchasing behavior, income and other relevant features. The goal of customer segmentation is to identify groups of customers with similar characteristics and needs, in order to develop targeted marketing strategies and improve customer satisfaction. The dataset used in this project is provided by Kaggle platform and includes information such as customer income, age, purchase history,customer feedback, etc. The clustering algorithms used are K-means Clustering and Hierarchical Clustering. The effectiveness of each algorithm will be evaluated based on metrics such as silhouette score and within-cluster sum of squares. The results of this project will provide insights into customer behavior and preferences, which can be used to tailor marketing campaigns and improve customer retention." 
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<div class="list-group" id="list-tab" role="tablist">
<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:160%;text-align:center;border-radius:10px 10px;">TABLE OF CONTENTS</p>

<br>
    
1. Introduction: Scope and purpose of your project

2. Data Manipulation
    
3. Descriptive Statistics
    
4. Clustering Methods
    
5. Summary

<br/>

--- 

<br>

<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:150%;text-align:center;border-radius:10px 10px;">Introduction: Scope and purpose of your project</p>

<br/>

The <b>objective</b> of this project is to conduct <u>customer segmentation</u> using <u>clustering analysis</u> to provide a concise summary of customer groups in order for companies to focus their marketing efforts accordingly.

The <b>dataset</b> used for this project consists of information about customers of a company. It includes <u>demographic information</u>such as the customer's unique ID, birth year, education level, marital status and yearly household income. Additionally, it includes information about the customer's <u>household composition</u> such as the number of children and teenagers living in the household. The dataset also provides information about the customer's <u>purchasing behavior</u>, including the amount spent on wine, fruit, meat, fish, sweets, and gold over the past two years. It includes information about promotions such as the number of purchases made with a discount, and whether the customer accepted offers from previous campaigns. Finally, it includes information about the customer's <u>purchasing habits</u>, such as the number of purchases made through the company's website, using a catalogue and directly in stores, as well as the number of visits to the company's website in the last month. This dataset could be used to develop targeted marketing strategies and to better understand customer behavior and preferences. More information about the dataset you can find [here](http://www.kaggle.com/datasets/imakash3011/customer-personality-analysis).

<i><b>Customer Segmentation</b></i> is one of the most important applications of unsupervised learning. Using clustering techniques, companies can categorize the customer base into different groups based on various relevant factors for marketing purposes

<i><b>Clustering</b></i> is one of the important data mining methods for discovering knowledge in multidimensional data. The goal of clustering is to identify pattern or groups of similar objects within a data set of interest. In the literature, it is referred as pattern recognition or unsupervised machine learning.

The standard process of clustering can be divided into the following several steps : 

* <b>Feature extraction and selection</b> -> extract and select the most representative features from the original data set
* <b>Clustering algorithm design</b> -> design the clustering algorithm according to the characteristics of the problem 
* <b>Result evaluation</b> -> evaluate the clustering result and judge the validity of algorithm 
* <b>Result explanation</b> -> give a practical explanation for the clustering result

When choosing a clustering algorithm, you should consider <u><b>whether the algorithm scales to your dataset</b></u>.

Many clustering algorithms work by computing the similarity between all pairs of examples (this means their runtime increases as the square of the number of examples n, denoted as O(n2) in complexity notation).

<br/>

<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:150%;text-align:center;border-radius:10px 10px;">Data Manipulation</p>

<br>

```{r}
# Import libraries
pacman :: p_load(pacman, stats, dplyr, ggplot2, ggExtra, GGally,corrplot, 
                 RColorBrewer, psych,graphics,lattice,caret, tidyr, fpc, 
                 data.table, DT, gridExtra, plotly, caret,cluster,RPMG, 
                 dendextend, dbscan)
                 

options(warn=-1) 
# warnings will be suppressed and will not 
# be displayed in the console or any output
```
<br/>

```{r}
# Loading data
data <- read.table("./marketing_campaign.csv", sep = "\t", header = T)
```

<br/>

```{r}
# Cleaning data
check <- function(data) {
  l <- list()
  columns <- names(data)
  for (col in columns) {
    instances <- sum(!is.na(data[[col]]))
    dtypes <- class(data[[col]])
    unique <- length(unique(data[[col]]))
    sum_null <- sum(is.na(data[[col]]))
    duplicates <- sum(duplicated(data))
    l[[length(l) + 1]] <- c(col, dtypes, instances, unique, 
                            sum_null, duplicates)
  }
  data_check <- as.data.frame(do.call(rbind, l))
  names(data_check) <- c("column", "dtype", "instances", "unique", 
                         "sum_null", "duplicates")
  return(data_check)
}

check(data)

# Remove the NA Values
data <- na.omit(data)
```

<br/>

```{r}
# Feature Engineering

# DateTime transformation
data$Dt_Customer <- as.Date(data$Dt_Customer, format="%d-%m-%Y")

# Tenure
data$Dt_Customer <- as.Date(data$Dt_Customer, format= "%d-%m-%Y")
days <- as.numeric(max(data$Dt_Customer) - data$Dt_Customer)
data$Tenure <- days
data$Tenure <- as.numeric(data$Tenure, errors="coerce")

# Age
data$Age <- 2023 - data$Year_Birth

# Spendings
data$Spendings <- data$MntWines + data$MntFruits + data$MntMeatProducts + 
  data$MntFishProducts + data$MntSweetProducts + data$MntGoldProds

    # Wines
    data$Wines <- data$MntWines

    # Fruits
    data$Fruits <- data$MntFruits

    # Meat
    data$Meat <- data$MntMeatProducts

    # Fish
    data$Fish <- data$MntFishProducts

    # Sweets
    data$Sweets <- data$MntSweetProducts

    # Gold
    data$Gold <- data$MntGoldProds

# Relatioship Status
data$RelationshipStatus <- ifelse(data$Marital_Status == "Married" | 
                             data$Marital_Status == "Together", "Partner",
ifelse(data$Marital_Status %in% c("Absurd", "Widow", "YOLO", 
                                  "Divorced", "Single"),"Alone", ""))

data$RelStatus <- ifelse(data$RelationshipStatus == "Partner", 2, 1)


# Children
data$Children <- data$Kidhome + data$Teenhome

# Parent
data$Parent <- ifelse(data$Children > 0, 1, 0)

# Education 
data$Education <- ifelse(data$Education %in% c("Basic", "2n Cycle"), 
                         "Undergraduate",
                         ifelse(data$Education == "Graduation", 
                                "Graduate",
                                ifelse(data$Education %in% c("Master", "PhD"), 
                                       "Postgraduate", "")))

data$LevEd <- as.numeric(ifelse(data$Education == "Undergraduate",1,
                             ifelse(data$Education == "Graduate", 2,
                                ifelse(data$Education == "Postgraduate", 3, 0))))
                                       
# Campaign
data$Campaign <- data$AcceptedCmp1 + data$AcceptedCmp2 + data$AcceptedCmp3 
+ data$AcceptedCmp4 + data$AcceptedCmp5

# Purchases
data$Purchases <- data$NumDealsPurchases + data$NumWebPurchases + 
  data$NumCatalogPurchases + data$NumStorePurchases

# change names of different variables for simplicity
data$WebVisits <- data$NumWebVisitsMonth
data$Web <- data$NumWebPurchases
data$Deal<- data$NumDealsPurchases
data$Catalog <- data$NumCatalogPurchases
data$Store <- data$NumStorePurchases

# Dropping some of the redundant features 
# and set "ds" as our clened data set for further analysis
to_drop <- c("Marital_Status", "NumDealsPurchases", "NumWebPurchases", "
             NumCatalogPurchases", "NumStorePurchases", "NumWebVisitsMonth",   
             "Dt_Customer","MntWines","MntFruits","MntMeatProducts","MntFishProducts",
             "MntSweetProducts","MntGoldProds","AcceptedCmp1","AcceptedCmp2", 
             "AcceptedCmp3", "AcceptedCmp4", "AcceptedCmp5","Z_CostContact", "
             Z_Revenue", "Year_Birth", "ID","Teenhome","Kidhome")
ds <- data[, !(names(data) %in% to_drop)]
check(ds)
```

<br/>

<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:150%;text-align:center;border-radius:10px 10px;">Descriptive Statistics</p>

<br/>

```{r}
# Generating a summary of all data attributes
describe(ds)
# summary(ds)
# do.call(cbind, lapply(ds, summary))
# summary(ds)
```

```{r}
options(repr.plot.width=20, repr.plot.height=10)
plots <- list()
variables <- c("Spendings", "Income", "Recency", "Tenure", "Age", "Wines", 
               "Fruits", "Meat", "Fish", "Sweets", "Gold", "Children", 
               "Purchases", "WebVisits", "Web", "Deal", "Catalog", "Store")

for (var in variables) {
  plot <- ggplot(ds, aes_string(x = var, fill = var)) +
    geom_boxplot(outlier.colour = "#A5D7E8", outlier.shape = 11, 
                 outlier.size = 2, col = "#0B2447", notch = F)
  plots[[var]] <- plot
}

grid.arrange(grobs = plots, ncol = 3)
```

```{r}
options(repr.plot.width=10, repr.plot.height=6) 
require(gridExtra)

Income_hist <- ggplot(ds, aes(x = Income)) +
  geom_histogram(color = '#0B2447', fill ='#7A86B6', bins=30) +
  labs(
    title = "Histogram for Income",
#     caption = "Source: Gapminder dataset",
    x = "Income",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#8A7546", size = 20, face = "bold"),
    plot.subtitle = element_text(color = "#D6B978",size = 12, face = "bold"),
    plot.caption = element_text(face = "italic"))

Spendings_hist <- ggplot(ds, aes(x = Spendings)) +
  geom_histogram(color = '#0B2447', fill ='#9F73AB', bins=30) +
  labs(
    title = "Histogram for Spendings",
#     caption = "Source: Gapminder dataset",
    x = "Spendings",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#8A7546", size = 20, face = "bold"),
    plot.subtitle = element_text(color = "#D6B978",size = 12, face = "bold"),
    plot.caption = element_text(face = "italic"))

grid.arrange(Income_hist, Spendings_hist, ncol=2)
```

```{r}
describe(ds$Income)
summary(ds$Income)

IQR_Income <- 68522 - 35303
upfen_Income <- 68522 + 1.5*IQR_Income
cat("\n","The upper fence for Income column would be:",upfen_Income)


describe(ds$Spendings)
summary(ds$Spendings)

IQR_Spendings <- 1048 - 69
upfen_Spendings <- 1048 + 1.5*IQR_Spendings
cat("\n","The upper fence for Spendings column would be:",upfen_Spendings)
```
```{r}
ds <- subset(ds, Income <= 118350.5 & Spendings <= 2516.5)
# check(ds)
cat("The total number of instances after removing the rows with outliers in 
    Income & Spendings columns is:", nrow(ds), "\n")
```
```{r}
options(repr.plot.width=16, repr.plot.height=12) 
par(mfrow = c(2, 2))

Income_hist <- hist(ds$Income,
  col="#7A86B6",
  main="Histogram for Income",
  xlab="Yearly Income",
  ylab="Frequency",
  labels=TRUE)

 Income_density <- plot(density(ds$Income),
    col="#7A86B6",
    main="Density Plot for Income",
    xlab="Yearly Income",
    ylab="Density")
polygon(density(ds$Income),
        col="#7A86B6")

Spendings_hist <- hist(ds$Spendings,
  col="#9F73AB",
  main="Histogram for Spendings",
  xlab="Monthly Spendings",
  ylab="Frequency",
  labels=TRUE)

 Spendings_density <- plot(density(ds$Spendings),
    col="#9F73AB",
    main="Density Plot for Spendings",
    xlab="Monthly Spendings",
    ylab="Density")
polygon(density(ds$Spendings),
        col="#9F73AB")

```

```{r}
options(repr.plot.width=16, repr.plot.height=8) 
require(gridExtra)

income_spendings_education_plot <- ggplot(ds, aes(x=Spendings,y=Income,
                                                  fill=Education)) +
geom_boxplot(outlier.colour="#0B2447", outlier.shape=16,outlier.size=2, notch=T) + 
scale_fill_manual(values = c("#980F5A", "#750550", "#570530")) +
scale_color_manual(values = c( "#0B2447"))

income_purchases_education_plot <- ggplot(ds, aes(x=Purchases,y=Income,fill=Education)) +
geom_boxplot(outlier.colour="#0B2447", outlier.shape=16,outlier.size=2, notch=T) + 
scale_fill_manual(values = c("#980F5A", "#750550", "#570530")) +
scale_color_manual(values = c( "#0B2447"))

purchases_hist <- ggplot(ds, aes(x=Purchases, fill=Education)) + 
  geom_histogram(color="black", bins = 30) +
  labs(x="Purchases", y="Frequency") +
  scale_fill_manual(values = c("#980F5A", "#750550", "#570530"))

income_hist <- ggplot(ds, aes(x=Income, fill=Education)) + 
  geom_histogram(color="black", bins = 30) +
  labs(x="Income", y="Frequency") +
  scale_fill_manual(values = c("#980F5A", "#750550", "#570530"))

grid.arrange(income_spendings_education_plot, income_purchases_education_plot,
             purchases_hist, income_hist, ncol=2, nrow=2)
```

</br>

<div class="alert alert-block alert-success">  

There are multiple categorical variables which we're going to drop in order to create a new dataframe <b>ds_num</b> to work (when needed) with functions that require dataframes containing just numerical values.   

</div>

```{r}
# Drop categorial variables 
drop <- c("Education", "RelationshipStatus")
ds_num <- ds[,!(names(ds) %in% drop)]
# check(num)
```

```{r}
# Between Predictors-Correlations

# Check correlation between all variables
options(repr.plot.width=20, repr.plot.height=10)
require(gridExtra)

plot1 <- ggcorr(ds, method = c("pairwise", "pearson"),hjust = 0.75, 
                size = 3.5, color = "#302e2e", layout.exp = 1, geom = "tile", 
                low = "steelblue", mid = "white", high = "darkred", 
                label = TRUE, label_size = 3, label_color = "#302e2e",
                label_alpha = TRUE)
# compare with other methods: c("complete", "pearson"),c("complete", "kendall"), c("all.obs", "spearman")
plot2 <- ggcorr(ds, method = c("pairwise", "pearson"),hjust = 0.75, 
                label = TRUE, hjust = 0.75, geom = "blank", layout.exp = 1) + 
                geom_point(size = 10, aes(color = coefficient > 0, 
                alpha = abs(coefficient) > 0.5), color = "#a86232") +
  scale_alpha_manual(values = c("TRUE" = 0.25, "FALSE" = 0)) +
  guides(color = FALSE, alpha = FALSE)
grid.arrange(plot1, plot2, ncol=2, nrow=1)
```

```{r}
options(repr.plot.width=20, repr.plot.height=10)
par(mfrow = c(1, 2))
plot3 <- corrplot(corr = cor(ds_num),method="color",order="hclust", tl.col = "#302e2e", type="upper",addCoef.col="#302e2e", number.cex=.7,number.digits=2,diag=TRUE, bg="#302e2e")
plot4 <- corrplot(cor(ds_num),type="full", tl.col = "#302e2e", order="hclust",number.digits=1,diag=TRUE, bg="grey",addrect=2, rect.col="#1c9123")
```

```{r}
# Check correlations (as scatterplots), distribution and print correlation coefficient 
options(repr.plot.width=20, repr.plot.height=20)
ggpairs(ds_num, title="Correlogram of full dataset variables")
```

</br>

<div class="alert alert-block alert-success"> 

There are 11 features that have been identified with high correlation with a cutoff set at 0.7: <b><i>Spendings</i></b>, <b><i>Income</i></b>, <b><i>Wines</i></b>, <b><i>Children</i></b>, <b><i>Purchases</i></b>, <b><i>Web</i></b>, <b><i>Parent</i></b>, <b><i>Meat</i></b>, <b><i>Store</i></b>,<b><i>Wines</i></b> and <b><i>Catalog</i></b>.

</div>

<br/>

<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:150%;text-align:center;border-radius:10px 10px;">Methods</p>

<br/>

My strategy for this assignment involves utilizing three algorithms: PCA, K-Means clustering, and Hierarchical Clustering. In this section, I will elaborate on the fundamental concepts underlying these techniques.

---

</br>

1. PCA

<b>Principal Components Analysis(PCA)</b> is an unsupervised learning method that is often used to <u>reduce the dimensionality</u> of a dataset by transforming a large set into a lower dimensional set that still contains most of the information of the large set. Is a procedure to transform linearly correlated variables into a set of new variables that are called <i> principal components</i> that are linearly correlated.

The goal is to find a transformation such that:
* the transformed features are <u>linearly independent</u>
* <u>dimensionality can be reduced</u> by taking only the dimensions with highest importance
* those newly found dimensions should <u> minimize the projection error</u>
* the projected points should have <u>maximum spread</u> (i.e.: maximum variance)

<b>Advice: PCA works best on continuous numeric data.</b>

The steps of the PCA algorithm are as follows:

* Step I: Data Preparation -> involves cleaning, normalizing and standardizing the data

* Step II: Compute the Covariance Matrix -> the covariance matrix is calculated by computing the pairwise covariance between the different features in the dataset

* Step III: Compute the Eigenvectors and Eigenvalues -> the eigenvectors are the directions in which the data varies the most and the eigenvalues indicate the amount of variance explained by each eigenvector (PCA finds the eigenvectors and eigenvalues of the covariance matrix)

* Step IV: Choose the Principal Components -> the eigenvectors are sorted in decreasing order of their eigenvalues; the eigenvectors with the highest eigenvalues, called the principal components(pc), represent the directions of maximum variance in the data

* Step V: Project the Data onto the Principal Components -> involves multiplying the original data matrix by the eigenvector matrix, which results in a new dataset that has fewer dimensions
<br/>

<div class="alert alert-block alert-success"> 
    
One of the main aims of PCA is to combine linearly correlated variables into a new set of variables that are called <i>principal components</i>, therefore the main prerequisites is to check whether the variables in the dataset are highly correlated amongst one another.

 The <b>Correlation Matrix</b> can help check whether or not the variables linearly dependent. If the average correlation is above 0.3 or below -0.3 we can say that there is evidence that the <u>variables are highly correlated amongst one another</u> and are hence <b>eligible for PCA</b>.
    
</div>

---

2. K-Means

</br>

<b>K-Means clustering</b> is a popular unsupervised machine learning algorithm used for grouping similar data points in a dataset into a specified number of clusters, with each cluster represented by its centroid. The algorithm works by iteratively assigning data points to their closest cluster centroid and then recalculating the centroid of each cluster based on the newly assigned data points. The process is repeated until the clusters stabilize or until a specified maximum number of iterations is reached.

A <b><i>good cluster</i></b> is one for which the within-cluster variation is as small as possible.


<b>Advice: K is a user-specified parameter and requires some domain knowledge or trial and error.</b>

The steps of the K-Means algorithm are as follows:

* Step I: Choose the number of clusters (K) -> determine the number of clusters (K) that the algorithm will use to group the data

* Step II: Initialize the centroids -> randomly select K data points as the initial centroids for each cluster

* Step III: Assign data points to the nearest centroid -> for each data point, calculate the distance to each centroid and assign it to the nearest centroid

* Step IV: Update the centroids -> calculate the mean of all data points assigned to each centroid and update the centroid position

* Step V:Repeat steps III and IV until the centroids no longer move significantly or a maximum number of iterations is reached

The algorithm may <b>converge to a local optimum</b>, meaning that the final clusters may depend on the initial random initialization of the centroids. Therefore, it is common practice to <u>run the algorithm multiple times</u> with different initial centroids and choose the solution with the <u>lowest overall within-cluster sum of squares (WSS) as the final result</u>.

There are several ways to check the performance of K-means clustering algorithm. One of them is <i>Within-Cluster Sum of Squares (WCSS)</i> method which calculates the WCSS for each cluster solution, plot them against the number of clusters and then looks for an <i>elbow</i> in the plot, which suggests the optimal number of clusters. 

Another way to check the performance of a K-Means algorithm is <i>Silhouette Score</i> method which measures how similar an object is to its own cluster compared to other clusters. The score ranges from -1 to 1, where higher values indicate better clustering.


---


3. Hierarchical Clustering

</br>

<b> Hierarchical clustering(HC) </b> is an unsupervised machine learning algorithm used for grouping similar data points in a dataset into a hierarchy of clusters. In this algorithm, each data point is initially considered as its own cluster, and the algorithm iteratively combines clusters into larger ones until all data points belong to a single cluster.

Hierarchical clustering produces a dendrogram that displays the hierarchy of the clusters, with the height of the branches indicating the degree of similarity between clusters. The dendrogram can be used to determine the optimal number of clusters to use for downstream analysis.

<b>Advice: Hierarchical clustering is more suitable for datasets with mixed data types.</b>

The steps of the HC algorithm are as follows:

* Step I: Begin with each data point as a separate cluster 
* Step II: Compute the distance matrix -> calculate the distance or similarity between each pair of clusters using a distance metric(i.e.: Euclidean distance, Manhattan distance,correlation coefficient, etc.)
* Step III: Merge the closest clusters -> find the closest pair of clusters based on the distance matrix and merge them into a single cluster
* Step IV: Recalculate the distance matrix -> update the distance matrix to reflect the newly formed cluster by recalculating the distance or similarity between the merged cluster and the remaining clusters
* Step V: Repeat steps III and IV until all data points belong to a single cluster: Continue to merge the closest clusters and update the distance matrix until all data points are in the same cluster.

<b><i>Linkage method</i></b> is a technique used in hierarchical clustering to determine how clusters are merged or divided based on the similarity or distance between data points. The choice of linkage method depends on the research question, the goals of the analysis, and the characteristics of the data. It is often recommended to try different linkage methods and examine the resulting dendrograms to determine the best method for a given dataset.

There are several linkage methods commonly used in HC including:

<i>Ward linkage</i>: This method defines the distance between two clusters as the sum of squared differences between each point and the centroid of the merged cluster and is generally preferred when the goal is to form well-separated clusters of similar size

<i>Complete linkage</i>: This method defines the distance between two clusters as the maximum distance between any two points in the two clusters and is preferred when the goal is to capture distinct groups with a large difference in similarity.
    
<i>Average linkage</i>: This method defines the distance between two clusters as the average distance between all pairs of points in the two clusters and s preferred when the goal is to form more balanced clusters of similar size.

 <i>Single linkage</i>: This method defines the distance between two clusters as the shortest distance between any two points in the two clusters and is typically used when the goal is to identify long,thin clusters.

<br/>

<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:150%;text-align:center;border-radius:10px 10px;">Results and interpretation</p>

<br/>

# PCA

<div class="alert alert-block alert-success"> 
    
One of the main aims of PCA is to combine linearly correlated variables into a new set of variables that are called <i>principal components</i>, therefore the main prerequisites is to check whether the variables in the dataset are highly correlated amongst one another.

 The <b>Correlation Matrix</b> can help check whether or not the variables linearly dependent. If the average correlation is above 0.3 or below -0.3 we can say that there is evidence that the <u>variables are highly correlated amongst one another</u> and are hence <b>eligible for PCA</b>.
    
</div>

```{r}
# Check PCA eligibility
# cor(ds_num)
mean(cor(ds_num))
```

<div class="alert alert-block alert-success"> 
    
The mean of the correlation between the dataset variables is 15.96% which is small. In theory this indicates that the dataset is <b>NOT</b> eligible for Principal Components Analysis, but for insights we are going to continue trying it. 
    
</div>

```{r}
# Principal Component Analysis
PCA <- prcomp(ds_num, centre=TRUE, scale=TRUE)
```

<div class="alert alert-block alert-success"> 
    
There are two ways of <b>evaluating PCA</b>. First is to check if whether the principal components are capturing essence of the original variables and the second is to check if whether the principal components are independent or not.
</div>

```{r}
summary(PCA)
```

<div class="alert alert-block alert-success"> 
    
* The first principal component explains <b>35.01%</b> of the total variation in the dataset.   
* The second principal component explains <b>10.20%</b> of the total variation in the dataset.
* The third principal component explains <b>6.45%</b> of the total variation in the dataset.
* The fourth principal component explains <b>6.17%</b> of the total variation in the dataset.
* The fifth principal component explains <b>4.31%</b> of the total variation in the dataset.
* etc.
    

There are <b>seven</b> PCs that capture <b> 70% </b> of the variance in the data, <b>ten</b> PCs that capture <b>80%</b> of the variance in the data, <b>forteenth</b> PCs that capture <b>90%</b> and <b>twenty-second</b> PCs that capture <b>100%</b>.
</div>

```{r}
# Total Variance explained by each PC
var_explained = PCA$sdev^2 / sum(PCA$sdev^2)

# Scree plot
options(repr.plot.width=10, repr.plot.height=6)
qplot(c(1:24), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
```

```{r}
options(repr.plot.width=20, repr.plot.height=20) 
# Defaylt biplot
biplot(PCA, scale=0)
```

```{r}
# Proportion of variance explained (PVE)
pr.var <- PCA$sdev^2

# We can then compute the proportion of variance explained by each principal component
pve=pr.var/sum(pr.var)

# We can plot the PVE explained by each component, as well as the cumulative PVE
options(repr.plot.width=14, repr.plot.height=8) 
par(mfrow = c(1, 2))
plot(pve,
xlab="Principal Component",
ylab="Proportion of Variance Explained",
ylim=c(0,1),
type='b')

plot(cumsum(pve),
xlab="Principal Component",
ylab="Cumulative Proportion of Variance Explained",
ylim=c(0,1),
type='b')
```

<div class="alert alert-block alert-success"> 
    
The numbers below show that although the graphs based on PC1 and PC2 give some insight about document types, <u>the first two PCs explain only small portion of the variability contained in the data, respectively <b>45.21%</b>.</u>
</div>

```{r}
# Obtain principal components
PC <- PCA$x
PC1 <- PC[, 1]
PC2 <- PC[, 2]
PC3 <- PC[, 3]

# Print first few rows of scores
# head(PC)

# Calculate correlation matrix
# cor(PC)
```

</br>

# K-Means Clustering

</br>

<div class="alert alert-block alert-success"> 
    
In order to determine the optimal value of k for our dataset, we will experiment with different values using a trial and error approach.
    
</div>    

```{r}
# Fit k-means clustering with k=2
km_out_2 <- kmeans(ds_num, 2)
# km_out_2

# Add cluster assignments to original data
ds_num_clustered <- cbind(ds_num, cluster = as.factor(km_out_2$cluster))

# Plot clustered data using ggplot2
options(repr.plot.width=12, repr.plot.height=6) 
require(gridExtra)

ds_num_k2 <- ggplot(ds_num_clustered, aes(x = Income, y = Spendings, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=2)", x = "Income", y = "Spendings")

ds_num_k2_pc <- ggplot(ds_num_clustered, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=2) within first 2 Principal Components", x = "PC1", y = "PC2")

grid.arrange(ds_num_k2, ds_num_k2_pc, ncol=2)
```

```{r}
# Fit k-means clustering with k=3
km_out_3 <- kmeans(ds_num, 3)
# km_out_3

# Add cluster assignments to original data
ds_num_clustered <- cbind(ds_num, cluster = as.factor(km_out_3$cluster))

# Plot clustered data using ggplot2
options(repr.plot.width=12, repr.plot.height=6) 
require(gridExtra)

ds_num_k3 <- ggplot(ds_num_clustered, aes(x = Income, y = Spendings, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=3)", x = "Income", y = "Spendings")

ds_num_k3_pc <- ggplot(ds_num_clustered, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=3) within first 2 Principal Components", x = "PC1", y = "PC2")

grid.arrange(ds_num_k3, ds_num_k3_pc, ncol=2)
```

```{r}
# Fit k-means clustering with k=4
km_out_4 <- kmeans(ds_num, 4)
# km_out_4

# Add cluster assignments to original data
ds_num_clustered <- cbind(ds_num, cluster = as.factor(km_out_4$cluster))

# Plot clustered data using ggplot2
options(repr.plot.width=12, repr.plot.height=6) 
require(gridExtra)

ds_num_k4 <- ggplot(ds_num_clustered, aes(x = Income, y = Spendings, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2","#B4E4FF")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=4)", x = "Income", y = "Spendings")

ds_num_k4_pc <- ggplot(ds_num_clustered, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2","#B4E4FF")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=4) within first 2 Principal Components", x = "PC1", y = "PC2")

grid.arrange(ds_num_k4, ds_num_k4_pc, ncol=2)
```

```{r}
# Fit k-means clustering with k=5
km_out_5 <- kmeans(ds_num, 5)
# km_out_5

# Add cluster assignments to original data
ds_num_clustered <- cbind(ds_num, cluster = as.factor(km_out_5$cluster))

# Plot clustered data using ggplot2
options(repr.plot.width=12, repr.plot.height=6)  
require(gridExtra)

ds_num_k5 <- ggplot(ds_num_clustered, aes(x = Income, y = Spendings, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2","#B4E4FF", "#C1AEFC")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=5)", x = "Income", y = "Spendings")

ds_num_k5_pc <- ggplot(ds_num_clustered, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2","#B4E4FF", "#C1AEFC")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=5) within first 2 Principal Components", x = "PC1", y = "PC2")

grid.arrange(ds_num_k5, ds_num_k5_pc, ncol=2)
```

```{r}
# Fit k-means clustering with k=6
km_out_6 <- kmeans(ds_num, 6)
# km_out_6

# Add cluster assignments to original data
ds_num_clustered <- cbind(ds_num, cluster = as.factor(km_out_6$cluster))

# Plot clustered data using ggplot2
options(repr.plot.width=12, repr.plot.height=6)  
require(gridExtra)

ds_num_k6 <- ggplot(ds_num_clustered, aes(x = Income, y = Spendings, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2", "#B4E4FF", "#C1AEFC", "#FFD495")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=6)", x = "Income", y = "Spendings")

ds_num_k6_pc <- ggplot(ds_num_clustered, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2", "#B4E4FF", "#C1AEFC", "#FFD495")) +
  theme_minimal() +
  labs(title = "Clustered Data (k=6) within first 2 Principal Components", x = "PC1", y = "PC2")

grid.arrange(ds_num_k6, ds_num_k6_pc, ncol=2)
```

<div class="alert alert-block alert-success"> 
    
<b>WSS</b> (Within-cluster Sum of Squares) measures the total distance between each data point and its assigned cluster centroid, summed over all clusters. By computing the WSS for various values of k and plotting the results, we can determine the optimal number of clusters to use for a given dataset. The idea is to choose a value of k that provides a good balance between having small WSS values (which indicates tight clustering) and not having too many clusters (which can lead to overfitting).

</div>

```{r}
# Compute WSS for various values of k
wss <- sapply(1:24, function(k){kmeans(ds_num, k, nstart=10 )$tot.withinss})

# Plot the wss for each value of k
ggplot(data.frame(x=1:24, wss=wss), aes(x=x, y=wss)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=3, linetype=2)
```

<div class="alert alert-block alert-success"> 

The resulting plot of WSS values for each value of k can be used to visually identify the <b><i>elbow point</i></b> which is the point on the plot where the rate of decrease in WSS starts to slow down. This point represents a good trade-off between small WSS values and a small number of clusters, and is often chosen as the optimal number of clusters. In our situation, a vertical line is added to the plot at <b>x=3</b> to indicate a possible elbow point, therefore we will use three clusters for our analysis.

</div>

```{r}
# Cluster assignments for k=3 model
# km_out_3
options(repr.plot.width=20, repr.plot.height=20) 

# Assign cluster labels based on cluster assignments
cluster_labels <- ifelse(km_out_3$cluster == 1, "Cluster 1",
                         ifelse(km_out_3$cluster == 2, "Cluster 2", "Cluster 3"))

# Plot the data with colored clusters and cluster labels
cluster_colors <- c("#B9F3E4", "#EA8FEA", "#F6E6C2")
plot(ds_num,col=cluster_colors[km_out_3$cluster],cex=0.5,pch=1,lwd=2)
text(ds_num, labels = cluster_labels, col = km_out_3$cluster)
```

</br>

# Hierarchical Clustering

</br>
 
 <div class="alert alert-block alert-success"> 
 
We are going to construct dendrograms by using <b>Euclidean</b> method for calculating the distance between observations and different linkage methods like <b>ward.D</b>, <b>complete</b>, <b>average</b> & <b>single</b>.
  
</div>

```{r}
Hierarchical_Cluster_distances <- dist(ds_num, method="euclidean")

# Ward.D Method
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method="ward.D")

# Complete Method
Hierarchical_Cluster_Complete <- hclust(Hierarchical_Cluster_distances, method="complete")

# Average Method
Hierarchical_Cluster_Average <- hclust(Hierarchical_Cluster_distances, method="average")

# Single Method
Hierarchical_Cluster_Single <- hclust(Hierarchical_Cluster_distances, method="single")

options(repr.plot.width=20, repr.plot.height=20) 
par(mfrow=c(2,2))
as.dendrogram(Hierarchical_Cluster) %>% color_branches(k=3) %>% plot(horiz=FALSE, main = "Ward.D Linkage") 
as.dendrogram(Hierarchical_Cluster_Complete) %>% color_branches(k=3) %>% plot(horiz=FALSE, main = "Complete Linkage") 
as.dendrogram(Hierarchical_Cluster_Complete) %>% color_branches(k=3) %>% plot(horiz=FALSE, main = "Average Linkage") 
as.dendrogram(Hierarchical_Cluster_Single) %>% color_branches(k=3) %>% plot(horiz=FALSE, main = "Single Linkage") 
```

<div class="alert alert-block alert-success"> 

Each dendrogram shows how the data points are grouped into clusters at different levels of similarity. The vertical height of the branches represents the distance between clusters or data points, while the horizontal lines represent the clusters or individual data points.The dendrograms are colored based on the number of clusters, which is set to 3 in this case.

</div>

<div class="alert alert-block alert-success"> 

Next we are going to set the number of clusters to 4. 

</div>

```{r}
options(repr.plot.width=20, repr.plot.height=20) 
par(mfrow=c(2,2))
as.dendrogram(Hierarchical_Cluster) %>% color_branches(k=4) %>% plot(horiz=FALSE, main = "Ward.D Linkage") 
as.dendrogram(Hierarchical_Cluster_Complete) %>% color_branches(k=4) %>% plot(horiz=FALSE, main = "Complete Linkage") 
as.dendrogram(Hierarchical_Cluster_Complete) %>% color_branches(k=4) %>% plot(horiz=FALSE, main = "Average Linkage") 
as.dendrogram(Hierarchical_Cluster_Single) %>% color_branches(k=4) %>% plot(horiz=FALSE, main = "Single Linkage") 
```

<div class="alert alert-block alert-success"> 

Next we are going to set the number of clusters to 6.

</div>

```{r}
options(repr.plot.width=20, repr.plot.height=20) 
par(mfrow=c(2,2))
as.dendrogram(Hierarchical_Cluster) %>% color_branches(k=6) %>% plot(horiz=FALSE, main = "Ward.D Linkage") 
as.dendrogram(Hierarchical_Cluster_Complete) %>% color_branches(k=6) %>% plot(horiz=FALSE, main = "Complete Linkage") 
as.dendrogram(Hierarchical_Cluster_Complete) %>% color_branches(k=6) %>% plot(horiz=FALSE, main = "Average Linkage") 
as.dendrogram(Hierarchical_Cluster_Single) %>% color_branches(k=6) %>% plot(horiz=FALSE, main = "Single Linkage") 
```

<div class="alert alert-block alert-success"> 
    
The computed dendrograms using the Ward.D method, tends to produce well-separated clusters of similar size, while using the Complete method tends to produce clusters that are more spread out and uneven in size. The computed dendrograms using the Average method produces clusters that are more balanced in size compared to the Complete method and ones computed using the Single method tends to produce long and thin clusters that are not relevant in this case.

</div>

```{r}
num <- nrow(ds_num) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
options(repr.plot.width=8, repr.plot.height=6) 
ggplot(df1[c(1:22),], aes(x=index, y=distances)) + geom_line() + xlab("Number of Components") +ylab("Distances") + theme_bw()
```

<div class="alert alert-block alert-success"> 

The elbow curve starts at three or six, which suggest that the cutoff for Hierarchical Clustering should be three or six. We are going to keep it at three going forward.

</div>

```{r}
# Cluster assignment
hc_clusters=cutree(Hierarchical_Cluster, 3)

# Add cluster assignments to original data
ds_num_hc_clustered <- cbind(ds_num, cluster = as.factor(hc_clusters))

# Plot clustered data using ggplot2
options(repr.plot.width=14, repr.plot.height=14)  
require(gridExtra)

ds_num_hc3 <- ggplot(ds_num_hc_clustered, aes(x = Income, y = Spendings, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2", "#B4E4FF", "#C1AEFC", "#FFD495")) +
  theme_minimal() +
  labs(title = "Clustered Data (hc=4)", x = "Income", y = "Spendings")

ds_num_hc3_pc <- ggplot(ds_num_hc_clustered, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.8, size = 3) +
  scale_color_manual(values = c("#B9F3E4", "#EA8FEA", "#F6E6C2", "#B4E4FF", "#C1AEFC", "#FFD495")) +
  theme_minimal() +
  labs(title = "Clustered Data (hc=4) within first 2 Principal Components", x = "PC1", y = "PC2")

grid.arrange(ds_num_hc3, ds_num_hc3_pc, ncol=2)
```

```{r}
# Cluster assignments for hc=3 model
options(repr.plot.width=20, repr.plot.height=20) 

# Assign cluster labels based on cluster assignments
cluster_labels <- ifelse(hc_clusters == 1, "Cluster 1",
                         ifelse(hc_clusters == 2, "Cluster 2", "Cluster 3"))

# Plot the data with colored clusters and cluster labels
cluster_colors <- c("#B9F3E4", "#EA8FEA", "#F6E6C2")
plot(ds_num,col=cluster_colors[hc_clusters],cex=0.5,pch=1,lwd=2)
text(ds_num, labels = cluster_labels, col = hc_clusters)
```

<div class="alert alert-block alert-success"> 
The fact that both the k-means and hierarchical clustering methods suggested that 3 clusters were appropriate is a strong indication that this is a meaningful and informative way to segment the customer data. The final result suggests that there are <b>three distinct subgroups within the customer population that have unique characteristics</b>. Unfortunately, as the variables of the dataset are not eligible for PCA, we can plot the clusters against some relevant variables such as <i>Income</i>, <i>Spendings</i>, <i>Purchases</i>.

</div>

```{r}
grid.arrange(ds_num_hc3, ds_num_hc3_pc,ds_num_k3,ds_num_k3_pc, ncol=2)
```

<div class="alert alert-block alert-success"> 
The agreement between the two clustering methods reinforces the credibility of this segmentation. This implies that the 3-cluster solution is not a mere result of a particular algorithm or methodology, but instead a strong and reliable conclusion that is likely to remain consistent across various methods and examinations.
</div>

<br/>

<p style="background-color:#5ac8db;font-family:newtimeroman;color:#03212e;font-size:150%;text-align:center;border-radius:10px 10px;">Summary</p>

<b>Note:</b> Scaling is an important preprocessing step in clustering that helps ensure that the different variables used in the analysis are on similar scales. When clustering is performed without scaling, variables with larger ranges or variances may have a disproportionate impact on the results compared to variables with smaller ranges or variances whick was not the case in our situation. The calculations were conducted on both unscaled and scaled data, with no significant variations in the outcomes.

In conclusion, the choice between K-means and HC depends on the nature of the data, the research question, and the computational resources available. K-means may be more suitable for large datasets and a fixed number of clusters, while hierarchical clustering may be more appropriate for smaller datasets and exploring the hierarchical relationships between clusters.

In our case, HC would be more suitablse because the dataset contains a mix of data types, even if could be computationally inefficient as the number of observations increases.

Also, HC can be more interpretable since the dendrogram provides a graphical representation of the clustering process and the hierarchical relationships between the clusters. K-means, on the other hand, provides less insight into the structure of the data and the relationships between clusters.

<b>Hence, based on our dataset, the optimal model fit using hierarchical clustering involves utilizing Euclidean distance and ward.D linkage method to divide our data into three distinct clusters.</b>

The next step would be to further investigate the characteristics of each of the three clusters and to identify what distinguishes them from one another. This could involve examining the variables that were used in the clustering analysis, as well as additional data or information about the customers and their behaviors or preferences. By understanding the unique characteristics of each cluster, businesses can tailor their marketing strategies, product offerings, or other business decisions to better meet the needs and preferences of different customer segments.